{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxDome Notebooks\n",
    "\n",
    "MaxDome notebooks are Serverless PySpark notebooks powered by AWS Glue Interactive Sessions.\n",
    "To Create a session and begin using PySpark simply run a cell of code. \n",
    "\n",
    "## Additional Sample Notebooks\n",
    "\n",
    "Additonal sample notebooks are available on the aws-glue-sample repo in GitHub and can be imported to your project using the upload button on the top of the file browser in Jupyter.\n",
    "\n",
    "https://github.com/aws-samples/aws-glue-samples/tree/master/examples/notebooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Configuration\n",
    "\n",
    "MaxDome notebooks are configured via Jupyter magics (commands prefixed with `%` and `%%`). In MaxDome and Glue IS, these are used to configure the PySpark environment including cluster size, shape and installed libraries. \n",
    "\n",
    "run `%help` in an empty cell to see the full list of magic commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Start Session and configure Spark\n",
    "import sys\n",
    "import boto3\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "args = getResolvedOptions(\n",
    "    sys.argv, [\"redshift_url\", \"redshift_iam_role\", \"redshift_tempdir\"]\n",
    ")\n",
    "\n",
    "# These exist in Sessions automatically. Adding for Linter\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get the MaxDome Database info\n",
    "glue_client = boto3.client(\"glue\")\n",
    "\n",
    "# filter Glue Databases for databases that start with \"maxdome_producer_db\"\n",
    "databases_paginator = glue_client.get_paginator(\"get_databases\")\n",
    "response_iterator = databases_paginator.paginate()\n",
    "glue_databases = response_iterator.build_full_result().get(\"DatabaseList\")\n",
    "maxdome_database = [\n",
    "    db for db in glue_databases if db[\"Name\"].startswith(\"maxdome_producer_db\")\n",
    "][0]\n",
    "\n",
    "maxdome_database_name = maxdome_database[\"Name\"]\n",
    "maxdome_database_location = maxdome_database[\"LocationUri\"]\n",
    "print(f\"Project Database Name: {maxdome_database_name}\")\n",
    "print(f\"Project Database Location: {maxdome_database_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark DataFrame from Titanic Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(f\"s3://sagemaker-example-files-prod-{boto3.session.Session().region_name}/datasets/tabular/dirty-titanic/\", header=True)\n",
    "df.show(5, truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "types_corrected = df.select(\n",
    "    col(\"index\").cast(\"integer\"),\n",
    "    col(\"survived\").cast(\"boolean\"),\n",
    "    col(\"name\").cast(\"string\"),\n",
    "    col(\"sex\").cast(\"string\"),\n",
    "    col(\"age\").cast(\"integer\"),\n",
    "    col(\"sibsp\").cast(\"float\"),\n",
    "    col(\"parch\").cast(\"float\"),\n",
    "    col(\"ticket\").cast(\"integer\"),\n",
    "    col(\"fare\").cast(\"double\"),\n",
    "    col(\"cabin\").cast(\"string\"),\n",
    "    col(\"embarked\").cast(\"string\"),\n",
    "    col(\"boat\").cast(\"integer\"),\n",
    "    col(\"`home.dest`\").cast(\"string\").alias(\"home_and_dest\"),\n",
    "    col(\"month_of_departure\").cast(\"string\"),\n",
    ")\n",
    "types_corrected.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data using the pandas API on Spark\n",
    "\n",
    "\n",
    "For those new to or unfamiliar with PySpark the Pandas API on Spark may allow for a more familiar interface. Using the library much of the operations on a Spark DataFrame can be executed using familiar pandas APIs while taking advantage of many of Spark's optimizations and distributed compute. \n",
    "\n",
    "See the [Spark documentation](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html) for a deeper dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# enable the pandas API on the Spark Dataframe\n",
    "ps_df = types_corrected.pandas_api()\n",
    "\n",
    "# Fill missing age\n",
    "ps_df[\"age\"] = ps_df[\"age\"].fillna(ps_df[\"age\"].astype(int).median())\n",
    "\n",
    "# Drop null survived\n",
    "ps_df = ps_df[ps_df[\"survived\"].notnull()]\n",
    "\n",
    "# Split Home and dest into two columns\n",
    "ps_df[[\"home\", \"dest\"]] = ps_df[\"home_and_dest\"].str.split(\"/\", n=1, expand=True)\n",
    "ps_df = ps_df.drop([\"home_and_dest\"], axis=1)\n",
    "\n",
    "\n",
    "# Convert back to a Spark Dataframe for write\n",
    "spark_df = ps_df.to_spark()\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the dataframe to a catalog table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing to the data catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "DATABASE = maxdome_database_name\n",
    "TABLE = \"titanic_clean\"\n",
    "S3_PATH = f\"{maxdome_database_location}{TABLE}/\"\n",
    "(spark_df.write.mode(\"append\").option(\"path\", S3_PATH).saveAsTable(f\"{DATABASE}.{TABLE}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data back from Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.sql(f\"select * from {DATABASE}.{TABLE}\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift Operations\n",
    "\n",
    "Redshift connectivity is currently handled by the community Spark Connector for Redshift and is best suited for moving large amounts of data between Redshift and Spark for use with PySpark or pandas.\n",
    "> A library to load data into Spark SQL DataFrames from Amazon Redshift, and write them back to Redshift tables. Amazon S3 is used to efficiently transfer data in and out of Redshift, and JDBC is used to automatically trigger the appropriate COPY and UNLOAD commands on Redshift.\n",
    ">\n",
    "> This library is more suited to ETL than interactive queries, since large amounts of data could be extracted to S3 for each query execution. If you plan to perform many queries against the same Redshift tables then we recommend saving the extracted data in a format such as Parquet.\n",
    "\n",
    "Parameters and documentation can be found [on GitHub](https://github.com/spark-redshift-community/spark-redshift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to Redshift with IAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "rs_database = \"dev\"\n",
    "rs_table = \"project.titanic\"\n",
    "\n",
    "(\n",
    "    spark_df.write.format(\"io.github.spark_redshift_community.spark.redshift\")\n",
    "    .option(\"url\", args[\"redshift_url\"])\n",
    "    .option(\"dbtable\", rs_table)\n",
    "    .option(\"tempdir\", args[\"redshift_tempdir\"])\n",
    "    .option(\"aws_iam_role\", args[\"redshift_iam_role\"])\n",
    "    .mode(\"overwrite\")\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read from Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "rs_read_df = (\n",
    "    spark.read.format(\"io.github.spark_redshift_community.spark.redshift\")\n",
    "    .option(\"url\", args[\"redshift_url\"])\n",
    "    .option(\"aws_iam_role\", args[\"redshift_iam_role\"])\n",
    "    .option(\"tempdir\", args[\"redshift_tempdir\"])\n",
    "    .option(\"unload_s3_format\", \"PARQUET\")\n",
    "    .option(\"dbtable\", rs_table)\n",
    "    .load()\n",
    ")\n",
    "rs_read_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing additional Python modules\n",
    "\n",
    "To install additional Python modules, such as the SageMaker PySDK, run `%additional_python_modules` at the start of your session. Note that to install Python modules without S3, you'll need internet access from your MaxDome VPC. For more info, please refer to https://docs.aws.amazon.com/glue/latest/dg/manage-notebook-sessions.html#specify-default-modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%stop_session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
